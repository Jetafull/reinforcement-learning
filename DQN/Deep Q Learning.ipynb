{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "\n",
    "if \"../\" not in sys.path: \n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.train.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0624716  0.00866946 0.00680757 0.02634746]\n",
      " [0.0624716  0.00866946 0.00680757 0.02634746]]\n",
      "99.65021\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sample when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        # TODO: Populate replay memory!\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps - 1)])\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    print(\"Finished replay memory populating.\")\n",
    "    \n",
    "    # Record videos\n",
    "    #env= Monitor(env, directory=monitor_path, resume=True,\n",
    "    #            video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            if t % 100 == 0: \n",
    "                print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                        t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            # TODO: Implement!\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(len(action_probs), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # TODO: Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            # TODO: Calculate q values and targets\n",
    "            # TODO Perform gradient descent update\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, done_batch = map(np.array, zip(*batch))\n",
    "            q_value_next = target_estimator.predict(sess, next_state_batch)\n",
    "            targets = reward_batch + np.invert(done_batch).astype(np.float) * discount_factor * np.max(q_value_next, axis=1)\n",
    "            loss = q_estimator.update(sess, state_batch, action_batch, targets)\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    #env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "Finished replay memory populating.\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 200 (200) @ Episode 1/10000, loss: 0.0017912550829350948\n",
      "Episode Reward: 1.0\n",
      "Step 300 (526) @ Episode 2/10000, loss: 0.0013846938963979483\n",
      "Episode Reward: 4.0\n",
      "Step 300 (904) @ Episode 3/10000, loss: 0.0014943639980629086\n",
      "Episode Reward: 3.0\n",
      "Step 200 (1164) @ Episode 4/10000, loss: 0.0015386443119496107\n",
      "Episode Reward: 2.0\n",
      "Step 200 (1452) @ Episode 5/10000, loss: 0.0015141521580517292\n",
      "Episode Reward: 1.0\n",
      "Step 100 (1583) @ Episode 6/10000, loss: 0.03100397437810898\n",
      "Episode Reward: 0.0\n",
      "Step 100 (1756) @ Episode 7/10000, loss: 0.00110358907841146\n",
      "Episode Reward: 0.0\n",
      "Step 200 (2037) @ Episode 8/10000, loss: 0.0005280803306959569\n",
      "Episode Reward: 1.0\n",
      "Step 100 (2162) @ Episode 9/10000, loss: 0.0008786292746663094\n",
      "Episode Reward: 0.0\n",
      "Step 200 (2429) @ Episode 10/10000, loss: 0.0005782184889540076\n",
      "Episode Reward: 2.0\n",
      "Step 200 (2707) @ Episode 11/10000, loss: 0.0006780331605114043\n",
      "Episode Reward: 1.0\n",
      "Step 200 (2944) @ Episode 12/10000, loss: 0.0005641673342324793\n",
      "Episode Reward: 1.0\n",
      "Step 300 (3275) @ Episode 13/10000, loss: 0.00043360615381971\n",
      "Episode Reward: 3.0\n",
      "Step 100 (3420) @ Episode 14/10000, loss: 0.0006638436461798847\n",
      "Episode Reward: 0.0\n",
      "Step 200 (3703) @ Episode 15/10000, loss: 0.00037454161792993546\n",
      "Episode Reward: 1.0\n",
      "Step 100 (3810) @ Episode 16/10000, loss: 0.03466632217168808\n",
      "Episode Reward: 0.0\n",
      "Step 200 (4088) @ Episode 17/10000, loss: 0.0005400502122938633\n",
      "Episode Reward: 1.0\n",
      "Step 100 (4227) @ Episode 18/10000, loss: 0.0347176194190979\n",
      "Episode Reward: 0.0\n",
      "Step 200 (4502) @ Episode 19/10000, loss: 0.0004891711287200451\n",
      "Episode Reward: 2.0\n",
      "Step 200 (4797) @ Episode 20/10000, loss: 0.0004472985747270286\n",
      "Episode Reward: 1.0\n",
      "Step 200 (5025) @ Episode 21/10000, loss: 0.001119800261221826\n",
      "Episode Reward: 1.0\n",
      "Step 200 (5264) @ Episode 22/10000, loss: 0.00048066244926303625\n",
      "Episode Reward: 1.0\n",
      "Step 200 (5500) @ Episode 23/10000, loss: 0.03154880926012993\n",
      "Episode Reward: 1.0\n",
      "Step 300 (5827) @ Episode 24/10000, loss: 0.0008466691942885518\n",
      "Episode Reward: 3.0\n",
      "Step 200 (6085) @ Episode 25/10000, loss: 0.0006577277090400457\n",
      "Episode Reward: 1.0\n",
      "Step 100 (6231) @ Episode 26/10000, loss: 0.000658145931083709\n",
      "Episode Reward: 0.0\n",
      "Step 300 (6625) @ Episode 27/10000, loss: 0.0006381557323038578\n",
      "Episode Reward: 3.0\n",
      "Step 200 (6859) @ Episode 28/10000, loss: 0.000772964209318161\n",
      "Episode Reward: 2.0\n",
      "Step 200 (7124) @ Episode 29/10000, loss: 0.00043211824959143996\n",
      "Episode Reward: 1.0\n",
      "Step 100 (7242) @ Episode 30/10000, loss: 0.0006321831606328487\n",
      "Episode Reward: 0.0\n",
      "Step 200 (7516) @ Episode 31/10000, loss: 0.030597567558288574\n",
      "Episode Reward: 2.0\n",
      "Step 100 (7709) @ Episode 32/10000, loss: 0.0008625834598205984\n",
      "Episode Reward: 0.0\n",
      "Step 200 (7987) @ Episode 33/10000, loss: 0.0005244766362011433\n",
      "Episode Reward: 1.0\n",
      "Step 400 (8436) @ Episode 34/10000, loss: 0.0009194941958412528\n",
      "Episode Reward: 5.0\n",
      "Step 100 (8570) @ Episode 35/10000, loss: 0.0005413174512796104\n",
      "Episode Reward: 0.0\n",
      "Step 200 (8844) @ Episode 36/10000, loss: 0.0002931589260697365\n",
      "Episode Reward: 1.0\n",
      "Step 200 (9055) @ Episode 37/10000, loss: 0.0003674337640404701\n",
      "Episode Reward: 2.0\n",
      "Step 100 (9229) @ Episode 38/10000, loss: 0.03212069720029831\n",
      "Episode Reward: 0.0\n",
      "Step 100 (9400) @ Episode 39/10000, loss: 0.0006599297630600631\n",
      "Episode Reward: 0.0\n",
      "Step 200 (9682) @ Episode 40/10000, loss: 0.0006326591828837991\n",
      "Episode Reward: 2.0\n",
      "Step 100 (9853) @ Episode 41/10000, loss: 0.0007144474657252431\n",
      "Episode Reward: 0.0\n",
      "Step 0 (9948) @ Episode 42/10000, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 200 (10148) @ Episode 42/10000, loss: 0.00047242807340808213\n",
      "Episode Reward: 2.0\n",
      "Step 100 (10336) @ Episode 43/10000, loss: 0.03163929283618927\n",
      "Episode Reward: 0.0\n",
      "Step 100 (10513) @ Episode 44/10000, loss: 0.00040191132575273514\n",
      "Episode Reward: 0.0\n",
      "Step 200 (10788) @ Episode 45/10000, loss: 0.0007142393733374774\n",
      "Episode Reward: 1.0\n",
      "Step 100 (10892) @ Episode 46/10000, loss: 0.0005998052074573934\n",
      "Episode Reward: 0.0\n",
      "Step 200 (11164) @ Episode 47/10000, loss: 0.0316644124686718\n",
      "Episode Reward: 2.0\n",
      "Step 200 (11407) @ Episode 48/10000, loss: 0.0005007467698305845\n",
      "Episode Reward: 1.0\n",
      "Step 100 (11547) @ Episode 49/10000, loss: 0.00046891183592379093\n",
      "Episode Reward: 0.0\n",
      "Step 100 (11716) @ Episode 50/10000, loss: 0.000571714190300554\n",
      "Episode Reward: 0.0\n",
      "Step 100 (11883) @ Episode 51/10000, loss: 0.00048563515883870423\n",
      "Episode Reward: 0.0\n",
      "Step 100 (12044) @ Episode 52/10000, loss: 0.03137531131505966\n",
      "Episode Reward: 0.0\n",
      "Step 100 (12224) @ Episode 53/10000, loss: 0.0008220481686294079\n",
      "Episode Reward: 0.0\n",
      "Step 200 (12499) @ Episode 54/10000, loss: 0.0005257244920358062\n",
      "Episode Reward: 2.0\n",
      "Step 300 (12893) @ Episode 55/10000, loss: 0.06585173308849335\n",
      "Episode Reward: 3.0\n",
      "Step 100 (13013) @ Episode 56/10000, loss: 0.0006112144328653812\n",
      "Episode Reward: 0.0\n",
      "Step 100 (13184) @ Episode 57/10000, loss: 0.0009027008200064301\n",
      "Episode Reward: 0.0\n",
      "Step 300 (13550) @ Episode 58/10000, loss: 0.0004836655280087143\n",
      "Episode Reward: 2.0\n",
      "Step 100 (13664) @ Episode 59/10000, loss: 0.0004429856489878148\n",
      "Episode Reward: 0.0\n",
      "Step 200 (13940) @ Episode 60/10000, loss: 0.0007373353000730276\n",
      "Episode Reward: 1.0\n",
      "Step 100 (14079) @ Episode 61/10000, loss: 0.0006958663580007851\n",
      "Episode Reward: 0.0\n",
      "Step 200 (24368) @ Episode 102/10000, loss: 0.0004644799919333309\n",
      "Episode Reward: 1.0\n",
      "Step 200 (24573) @ Episode 103/10000, loss: 0.031437426805496216\n",
      "Episode Reward: 1.0\n",
      "Step 200 (24825) @ Episode 104/10000, loss: 0.0006613580044358969\n",
      "Episode Reward: 1.0\n",
      "Step 200 (25064) @ Episode 105/10000, loss: 0.0009437109110876918\n",
      "Episode Reward: 2.0\n",
      "Step 200 (25356) @ Episode 106/10000, loss: 0.0010227907914668322\n",
      "Episode Reward: 1.0\n",
      "Step 100 (25493) @ Episode 107/10000, loss: 0.0006945573259145021\n",
      "Episode Reward: 0.0\n",
      "Step 200 (25761) @ Episode 108/10000, loss: 0.0008025308488868177\n",
      "Episode Reward: 1.0\n",
      "Step 100 (25896) @ Episode 109/10000, loss: 0.0008100727573037148\n",
      "Episode Reward: 0.0\n",
      "Step 200 (26162) @ Episode 110/10000, loss: 0.0004979846999049187\n",
      "Episode Reward: 2.0\n",
      "Step 300 (26543) @ Episode 111/10000, loss: 0.0008664482738822699\n",
      "Episode Reward: 2.0\n",
      "Step 200 (26758) @ Episode 112/10000, loss: 0.0007515676552429795\n",
      "Episode Reward: 1.0\n",
      "Step 300 (27109) @ Episode 113/10000, loss: 0.031434666365385056\n",
      "Episode Reward: 3.0\n",
      "Step 200 (27399) @ Episode 114/10000, loss: 0.000920620805118233\n",
      "Episode Reward: 1.0\n",
      "Step 200 (27633) @ Episode 115/10000, loss: 0.0007405205979011953\n",
      "Episode Reward: 2.0\n",
      "Step 200 (27912) @ Episode 116/10000, loss: 0.0006536237196996808\n",
      "Episode Reward: 2.0\n",
      "Step 200 (28208) @ Episode 117/10000, loss: 0.0012765703722834587\n",
      "Episode Reward: 1.0\n",
      "Step 300 (28531) @ Episode 118/10000, loss: 0.0009629613487049937\n",
      "Episode Reward: 3.0\n",
      "Step 200 (28790) @ Episode 119/10000, loss: 0.0010729944333434105\n",
      "Episode Reward: 1.0\n",
      "Step 300 (29150) @ Episode 120/10000, loss: 0.0009061766322702169\n",
      "Episode Reward: 2.0\n",
      "Step 200 (29352) @ Episode 121/10000, loss: 0.0011647436767816544\n",
      "Episode Reward: 1.0\n",
      "Step 200 (29577) @ Episode 122/10000, loss: 0.0009314125636592507\n",
      "Episode Reward: 1.0\n",
      "Step 100 (29724) @ Episode 123/10000, loss: 0.0007309780921787024\n",
      "Episode Reward: 0.0\n",
      "Step 100 (29891) @ Episode 124/10000, loss: 0.0014645527116954327\n",
      "Episode Reward: 0.0\n",
      "Step 0 (29968) @ Episode 125/10000, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 100 (30068) @ Episode 125/10000, loss: 0.0010437308810651302\n",
      "Episode Reward: 0.0\n",
      "Step 200 (30342) @ Episode 126/10000, loss: 0.0007170297321863472\n",
      "Episode Reward: 1.0\n",
      "Step 300 (30657) @ Episode 127/10000, loss: 0.0008857512148097157\n",
      "Episode Reward: 2.0\n",
      "Step 100 (30783) @ Episode 128/10000, loss: 0.0017179688438773155\n",
      "Episode Reward: 0.0\n",
      "Step 200 (31044) @ Episode 129/10000, loss: 0.0016713016666471958\n",
      "Episode Reward: 2.0\n",
      "Step 200 (31320) @ Episode 130/10000, loss: 0.0012969316449016333\n",
      "Episode Reward: 1.0\n",
      "Step 200 (31551) @ Episode 131/10000, loss: 0.001281454460695386\n",
      "Episode Reward: 1.0\n",
      "Step 500 (32054) @ Episode 132/10000, loss: 0.0015397958923131227\n",
      "Episode Reward: 7.0\n",
      "Step 300 (32434) @ Episode 133/10000, loss: 0.0002532947692088783\n",
      "Episode Reward: 3.0\n",
      "Step 100 (32551) @ Episode 134/10000, loss: 0.03027443028986454\n",
      "Episode Reward: 0.0\n",
      "Step 100 (32744) @ Episode 135/10000, loss: 0.0006014535902068019\n",
      "Episode Reward: 0.0\n",
      "Step 200 (33037) @ Episode 136/10000, loss: 0.0012496293056756258\n",
      "Episode Reward: 2.0\n",
      "Step 100 (33213) @ Episode 137/10000, loss: 0.031844135373830795\n",
      "Episode Reward: 0.0\n",
      "Step 100 (33403) @ Episode 138/10000, loss: 0.0014837936032563448\n",
      "Episode Reward: 0.0\n",
      "Step 100 (33573) @ Episode 139/10000, loss: 0.0015016006072983146\n",
      "Episode Reward: 1.0\n",
      "Step 100 (33772) @ Episode 140/10000, loss: 0.0008759919437579811\n",
      "Episode Reward: 0.0\n",
      "Step 200 (34044) @ Episode 141/10000, loss: 0.0014000124065205455\n",
      "Episode Reward: 1.0\n",
      "Step 100 (34185) @ Episode 142/10000, loss: 0.0014634437393397093\n",
      "Episode Reward: 0.0\n",
      "Step 200 (34461) @ Episode 143/10000, loss: 0.0012988368980586529\n",
      "Episode Reward: 1.0\n",
      "Step 300 (34788) @ Episode 144/10000, loss: 0.0016479722689837217\n",
      "Episode Reward: 3.0\n",
      "Step 100 (34933) @ Episode 145/10000, loss: 0.0012837168760597706\n",
      "Episode Reward: 0.0\n",
      "Step 200 (35211) @ Episode 146/10000, loss: 0.0014669728698208928\n",
      "Episode Reward: 2.0\n",
      "Step 300 (35566) @ Episode 147/10000, loss: 0.0015567708760499954\n",
      "Episode Reward: 3.0\n",
      "Step 300 (35924) @ Episode 148/10000, loss: 0.0008730856934562325\n",
      "Episode Reward: 3.0\n",
      "Step 300 (36264) @ Episode 149/10000, loss: 0.001567573519423604\n",
      "Episode Reward: 3.0\n",
      "Step 200 (36467) @ Episode 150/10000, loss: 0.0011753500439226627\n",
      "Episode Reward: 2.0\n",
      "Step 300 (36855) @ Episode 151/10000, loss: 0.0376431979238987\n",
      "Episode Reward: 3.0\n",
      "Step 300 (37172) @ Episode 152/10000, loss: 0.0010963149834424257\n",
      "Episode Reward: 3.0\n",
      "Step 100 (37343) @ Episode 153/10000, loss: 0.0012907369527965784\n",
      "Episode Reward: 0.0\n",
      "Step 300 (37716) @ Episode 154/10000, loss: 0.0013607644941657782\n",
      "Episode Reward: 4.0\n",
      "Step 100 (37856) @ Episode 155/10000, loss: 0.0005866102874279022\n",
      "Episode Reward: 0.0\n",
      "Step 200 (38136) @ Episode 156/10000, loss: 0.00321439141407609\n",
      "Episode Reward: 1.0\n",
      "Step 200 (38361) @ Episode 157/10000, loss: 0.0010243772994726896\n",
      "Episode Reward: 1.0\n",
      "Step 100 (38474) @ Episode 158/10000, loss: 0.001855471869930625\n",
      "Episode Reward: 0.0\n",
      "Step 100 (38647) @ Episode 159/10000, loss: 0.0012777980882674456\n",
      "Episode Reward: 0.0\n",
      "Step 300 (39017) @ Episode 160/10000, loss: 0.0006794165819883347\n",
      "Episode Reward: 4.0\n",
      "Step 400 (39506) @ Episode 161/10000, loss: 0.0017079164972528815\n",
      "Episode Reward: 4.0\n",
      "Step 100 (39606) @ Episode 162/10000, loss: 0.0007364588091149926\n",
      "Episode Reward: 0.0\n",
      "Step 200 (39897) @ Episode 163/10000, loss: 0.03786896541714668\n",
      "Episode Reward: 2.0\n",
      "Step 0 (39979) @ Episode 164/10000, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 200 (40179) @ Episode 164/10000, loss: 0.0010001998161897063\n",
      "Episode Reward: 2.0\n",
      "Step 100 (40360) @ Episode 165/10000, loss: 0.0012602228671312332\n",
      "Episode Reward: 0.0\n",
      "Step 300 (40748) @ Episode 166/10000, loss: 0.012279421091079712\n",
      "Episode Reward: 2.0\n",
      "Step 300 (41048) @ Episode 167/10000, loss: 0.0019191147293895483\n",
      "Episode Reward: 2.0\n",
      "Step 200 (41249) @ Episode 168/10000, loss: 0.0016143714310601354\n",
      "Episode Reward: 1.0\n",
      "Step 200 (41480) @ Episode 169/10000, loss: 0.0003079717280343175\n",
      "Episode Reward: 2.0\n",
      "Step 200 (41738) @ Episode 170/10000, loss: 0.0009756720392033458\n",
      "Episode Reward: 2.0\n",
      "Step 200 (42023) @ Episode 171/10000, loss: 0.0014059633249416947\n",
      "Episode Reward: 1.0\n",
      "Step 100 (42152) @ Episode 172/10000, loss: 0.00024589052191004157\n",
      "Episode Reward: 0.0\n",
      "Step 200 (42438) @ Episode 173/10000, loss: 9.156177111435682e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (42550) @ Episode 174/10000, loss: 0.00012934501864947379\n",
      "Episode Reward: 0.0\n",
      "Step 200 (42819) @ Episode 175/10000, loss: 5.472838529385626e-05\n",
      "Episode Reward: 1.0\n",
      "Step 300 (43125) @ Episode 176/10000, loss: 0.00259057292714715\n",
      "Episode Reward: 3.0\n",
      "Step 200 (43356) @ Episode 177/10000, loss: 7.791307871229947e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (43598) @ Episode 178/10000, loss: 0.026447057723999023\n",
      "Episode Reward: 1.0\n",
      "Step 100 (43722) @ Episode 179/10000, loss: 9.175870945909992e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (43998) @ Episode 180/10000, loss: 0.0022078410256654024\n",
      "Episode Reward: 1.0\n",
      "Step 100 (44101) @ Episode 181/10000, loss: 0.0006481597083620727\n",
      "Episode Reward: 0.0\n",
      "Step 100 (44285) @ Episode 182/10000, loss: 9.32336988626048e-05\n",
      "Episode Reward: 0.0\n",
      "Step 100 (44467) @ Episode 183/10000, loss: 0.0002666311338543892\n",
      "Episode Reward: 0.0\n",
      "Step 100 (44648) @ Episode 184/10000, loss: 0.00034494747524149716\n",
      "Episode Reward: 0.0\n",
      "Step 200 (44926) @ Episode 185/10000, loss: 4.6947767259553075e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (45176) @ Episode 186/10000, loss: 0.00013632632908411324\n",
      "Episode Reward: 2.0\n",
      "Step 200 (45440) @ Episode 187/10000, loss: 0.0003533967537805438\n",
      "Episode Reward: 1.0\n",
      "Step 100 (45574) @ Episode 188/10000, loss: 0.0020898703951388597\n",
      "Episode Reward: 0.0\n",
      "Step 200 (45869) @ Episode 189/10000, loss: 0.0001281556615140289\n",
      "Episode Reward: 2.0\n",
      "Step 400 (46353) @ Episode 190/10000, loss: 0.00035096268402412534\n",
      "Episode Reward: 4.0\n",
      "Step 200 (46559) @ Episode 191/10000, loss: 5.474906720337458e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (46670) @ Episode 192/10000, loss: 4.977366188541055e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (46944) @ Episode 193/10000, loss: 0.0006850678473711014\n",
      "Episode Reward: 1.0\n",
      "Step 200 (47187) @ Episode 194/10000, loss: 0.0024878752883523703\n",
      "Episode Reward: 1.0\n",
      "Step 200 (47422) @ Episode 195/10000, loss: 0.0037278463132679462\n",
      "Episode Reward: 1.0\n",
      "Step 400 (47870) @ Episode 196/10000, loss: 0.00047784007620066404\n",
      "Episode Reward: 6.0\n",
      "Step 300 (48259) @ Episode 197/10000, loss: 0.0006293111946433783\n",
      "Episode Reward: 3.0\n",
      "Step 200 (48473) @ Episode 198/10000, loss: 0.00032094138441607356\n",
      "Episode Reward: 1.0\n",
      "Step 200 (48704) @ Episode 199/10000, loss: 8.712300768820569e-05\n",
      "Episode Reward: 2.0\n",
      "Step 100 (48874) @ Episode 200/10000, loss: 4.7139415983110666e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (49145) @ Episode 201/10000, loss: 0.00022434425773099065\n",
      "Episode Reward: 1.0\n",
      "Step 100 (49280) @ Episode 202/10000, loss: 5.1321141654625535e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (49538) @ Episode 203/10000, loss: 4.250258280080743e-05\n",
      "Episode Reward: 1.0\n",
      "Step 300 (49897) @ Episode 204/10000, loss: 4.891004937235266e-05\n",
      "Episode Reward: 4.0\n",
      "Step 0 (49932) @ Episode 205/10000, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 200 (50132) @ Episode 205/10000, loss: 0.0006696783239021897\n",
      "Episode Reward: 1.0\n",
      "Step 200 (50380) @ Episode 206/10000, loss: 0.00019277411047369242\n",
      "Episode Reward: 2.0\n",
      "Step 100 (50579) @ Episode 207/10000, loss: 5.451009201351553e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (50851) @ Episode 208/10000, loss: 0.0018901932053267956\n",
      "Episode Reward: 1.0\n",
      "Step 200 (51066) @ Episode 209/10000, loss: 0.021846232935786247\n",
      "Episode Reward: 1.0\n",
      "Step 200 (51301) @ Episode 210/10000, loss: 0.003910357132554054\n",
      "Episode Reward: 1.0\n",
      "Step 200 (51541) @ Episode 211/10000, loss: 0.0010917973704636097\n",
      "Episode Reward: 2.0\n",
      "Step 200 (51834) @ Episode 212/10000, loss: 0.00022773994714953005\n",
      "Episode Reward: 2.0\n",
      "Step 200 (52115) @ Episode 213/10000, loss: 0.00012079453881597146\n",
      "Episode Reward: 1.0\n",
      "Step 100 (52256) @ Episode 214/10000, loss: 0.00045390627929009497\n",
      "Episode Reward: 0.0\n",
      "Step 100 (52436) @ Episode 215/10000, loss: 9.958301234291866e-05\n",
      "Episode Reward: 0.0\n",
      "Step 300 (52797) @ Episode 216/10000, loss: 0.003943384159356356\n",
      "Episode Reward: 2.0\n",
      "Step 200 (53011) @ Episode 217/10000, loss: 6.339277751976624e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (53259) @ Episode 218/10000, loss: 0.0013446344528347254\n",
      "Episode Reward: 2.0\n",
      "Step 200 (53514) @ Episode 219/10000, loss: 0.00034527044044807553\n",
      "Episode Reward: 1.0\n",
      "Step 300 (53835) @ Episode 220/10000, loss: 8.87986971065402e-05\n",
      "Episode Reward: 3.0\n",
      "Step 200 (54054) @ Episode 221/10000, loss: 8.086975140031427e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (54287) @ Episode 222/10000, loss: 9.604163642507046e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (54422) @ Episode 223/10000, loss: 3.936931898351759e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (54716) @ Episode 224/10000, loss: 3.687492062454112e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (54844) @ Episode 225/10000, loss: 0.00034211904858238995\n",
      "Episode Reward: 0.0\n",
      "Step 400 (55321) @ Episode 226/10000, loss: 0.0025609552394598722\n",
      "Episode Reward: 4.0\n",
      "Step 100 (55438) @ Episode 227/10000, loss: 0.00015945924678817391\n",
      "Episode Reward: 0.0\n",
      "Step 200 (55712) @ Episode 228/10000, loss: 6.718743679812178e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (55867) @ Episode 229/10000, loss: 0.00015102728502824903\n",
      "Episode Reward: 0.0\n",
      "Step 200 (56139) @ Episode 230/10000, loss: 0.00020498744561336935\n",
      "Episode Reward: 2.0\n",
      "Step 300 (56497) @ Episode 231/10000, loss: 0.00023562989372294396\n",
      "Episode Reward: 3.0\n",
      "Step 300 (56831) @ Episode 232/10000, loss: 0.00010075204772874713\n",
      "Episode Reward: 2.0\n",
      "Step 200 (57045) @ Episode 233/10000, loss: 0.00020811699505429715\n",
      "Episode Reward: 2.0\n",
      "Step 100 (57241) @ Episode 234/10000, loss: 0.00036399628152139485\n",
      "Episode Reward: 0.0\n",
      "Step 300 (57620) @ Episode 235/10000, loss: 0.0002555064857006073\n",
      "Episode Reward: 4.0\n",
      "Step 100 (57811) @ Episode 236/10000, loss: 3.892061795340851e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (58077) @ Episode 237/10000, loss: 3.608036786317825e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (58332) @ Episode 238/10000, loss: 6.879671855131164e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (58607) @ Episode 239/10000, loss: 0.00019199684902559966\n",
      "Episode Reward: 1.0\n",
      "Step 200 (58814) @ Episode 240/10000, loss: 0.0003903449105564505\n",
      "Episode Reward: 1.0\n",
      "Step 100 (58953) @ Episode 241/10000, loss: 0.0001202501734951511\n",
      "Episode Reward: 0.0\n",
      "Step 100 (59123) @ Episode 242/10000, loss: 0.00043288874439895153\n",
      "Episode Reward: 0.0\n",
      "Step 200 (59390) @ Episode 243/10000, loss: 0.00031582763767801225\n",
      "Episode Reward: 1.0\n",
      "Step 200 (59621) @ Episode 244/10000, loss: 2.5598128559067845e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (59751) @ Episode 245/10000, loss: 0.000384543149266392\n",
      "Episode Reward: 0.0\n",
      "Step 100 (59921) @ Episode 246/10000, loss: 4.211058694636449e-05\n",
      "Copied model parameters to target network.\n",
      "Step 200 (60021) @ Episode 246/10000, loss: 7.60794646339491e-05\n",
      "Episode Reward: 2.0\n",
      "Step 100 (60208) @ Episode 247/10000, loss: 0.003294065361842513\n",
      "Episode Reward: 0.0\n",
      "Step 200 (60486) @ Episode 248/10000, loss: 0.0009229991119354963\n",
      "Episode Reward: 2.0\n",
      "Step 200 (60749) @ Episode 249/10000, loss: 0.004126424435526133\n",
      "Episode Reward: 2.0\n",
      "Step 200 (61022) @ Episode 250/10000, loss: 0.01175867673009634\n",
      "Episode Reward: 1.0\n",
      "Step 300 (61334) @ Episode 251/10000, loss: 5.802639134344645e-05\n",
      "Episode Reward: 3.0\n",
      "Step 200 (61544) @ Episode 252/10000, loss: 0.0002595257537905127\n",
      "Episode Reward: 1.0\n",
      "Step 200 (61791) @ Episode 253/10000, loss: 0.0001442781212972477\n",
      "Episode Reward: 1.0\n",
      "Step 200 (62018) @ Episode 254/10000, loss: 0.0011557021643966436\n",
      "Episode Reward: 2.0\n",
      "Step 200 (62308) @ Episode 255/10000, loss: 3.9057085814420134e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (62551) @ Episode 256/10000, loss: 0.0001613836211618036\n",
      "Episode Reward: 1.0\n",
      "Step 200 (62786) @ Episode 257/10000, loss: 3.010804721270688e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (63024) @ Episode 258/10000, loss: 2.957965261884965e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (63170) @ Episode 259/10000, loss: 7.853259012335911e-05\n",
      "Episode Reward: 0.0\n",
      "Step 400 (63640) @ Episode 260/10000, loss: 0.00010634471254888922\n",
      "Episode Reward: 4.0\n",
      "Step 200 (63865) @ Episode 261/10000, loss: 9.04960252228193e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (64089) @ Episode 262/10000, loss: 0.00032266139169223607\n",
      "Episode Reward: 1.0\n",
      "Step 100 (64198) @ Episode 263/10000, loss: 6.58790668239817e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (64470) @ Episode 264/10000, loss: 2.9943243134766817e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (64740) @ Episode 265/10000, loss: 0.000399066018871963\n",
      "Episode Reward: 1.0\n",
      "Step 200 (64968) @ Episode 266/10000, loss: 0.0001641395065234974\n",
      "Episode Reward: 2.0\n",
      "Step 300 (65354) @ Episode 267/10000, loss: 0.0007935543544590473\n",
      "Episode Reward: 2.0\n",
      "Step 100 (65463) @ Episode 268/10000, loss: 5.2263032557675615e-05\n",
      "Episode Reward: 0.0\n",
      "Step 100 (65651) @ Episode 269/10000, loss: 0.0009434031089767814\n",
      "Episode Reward: 0.0\n",
      "Step 100 (65821) @ Episode 270/10000, loss: 5.9225658333161846e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (66101) @ Episode 271/10000, loss: 3.7058671296108514e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (66365) @ Episode 272/10000, loss: 8.912725752452388e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (66589) @ Episode 273/10000, loss: 0.0033379807136952877\n",
      "Episode Reward: 1.0\n",
      "Step 200 (66822) @ Episode 274/10000, loss: 3.1576935725752264e-05\n",
      "Episode Reward: 2.0\n",
      "Step 100 (67014) @ Episode 275/10000, loss: 0.0003837999247480184\n",
      "Episode Reward: 0.0\n",
      "Step 200 (67284) @ Episode 276/10000, loss: 0.00021652629948221147\n",
      "Episode Reward: 1.0\n",
      "Step 100 (67395) @ Episode 277/10000, loss: 5.1282906497363e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (67664) @ Episode 278/10000, loss: 0.0018092705868184566\n",
      "Episode Reward: 1.0\n",
      "Step 200 (67896) @ Episode 279/10000, loss: 0.0024748186115175486\n",
      "Episode Reward: 1.0\n",
      "Step 400 (68338) @ Episode 280/10000, loss: 2.9899925721110776e-05\n",
      "Episode Reward: 5.0\n",
      "Step 100 (68482) @ Episode 281/10000, loss: 0.0002060883562080562\n",
      "Episode Reward: 0.0\n",
      "Step 300 (68849) @ Episode 282/10000, loss: 0.0033389159943908453\n",
      "Episode Reward: 3.0\n",
      "Step 200 (69059) @ Episode 283/10000, loss: 0.00012052345846313983\n",
      "Episode Reward: 2.0\n",
      "Step 400 (69529) @ Episode 284/10000, loss: 6.638514605583623e-05\n",
      "Episode Reward: 4.0\n",
      "Step 200 (69739) @ Episode 285/10000, loss: 0.001467346795834601\n",
      "Episode Reward: 2.0\n",
      "Step 100 (69912) @ Episode 286/10000, loss: 0.0006731666508130729\n",
      "Copied model parameters to target network.\n",
      "Step 200 (70012) @ Episode 286/10000, loss: 3.155110607622191e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (70285) @ Episode 287/10000, loss: 6.476110138464719e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (70445) @ Episode 288/10000, loss: 5.206163405091502e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (70724) @ Episode 289/10000, loss: 0.0010481122881174088\n",
      "Episode Reward: 2.0\n",
      "Step 100 (70890) @ Episode 290/10000, loss: 8.578219421906397e-05\n",
      "Episode Reward: 0.0\n",
      "Step 300 (71264) @ Episode 291/10000, loss: 5.9100631915498525e-05\n",
      "Episode Reward: 3.0\n",
      "Step 200 (71501) @ Episode 292/10000, loss: 0.00016462753410451114\n",
      "Episode Reward: 1.0\n",
      "Step 200 (71750) @ Episode 293/10000, loss: 0.005334276705980301\n",
      "Episode Reward: 1.0\n",
      "Step 500 (72292) @ Episode 294/10000, loss: 0.0020486970897763968\n",
      "Episode Reward: 7.0\n",
      "Step 100 (72410) @ Episode 295/10000, loss: 0.00013954885071143508\n",
      "Episode Reward: 0.0\n",
      "Step 200 (72685) @ Episode 296/10000, loss: 0.00039183697663247585\n",
      "Episode Reward: 2.0\n",
      "Step 200 (72967) @ Episode 297/10000, loss: 1.6106989278341644e-05\n",
      "Episode Reward: 1.0\n",
      "Step 400 (73397) @ Episode 298/10000, loss: 0.0002686918596737087\n",
      "Episode Reward: 6.0\n",
      "Step 200 (73668) @ Episode 299/10000, loss: 0.00029600298148579895\n",
      "Episode Reward: 1.0\n",
      "Step 200 (73923) @ Episode 300/10000, loss: 0.0005622534663416445\n",
      "Episode Reward: 2.0\n",
      "Step 300 (74301) @ Episode 301/10000, loss: 0.00024285867402795702\n",
      "Episode Reward: 2.0\n",
      "Step 300 (74603) @ Episode 302/10000, loss: 0.000601121864747256\n",
      "Episode Reward: 4.0\n",
      "Step 200 (74883) @ Episode 303/10000, loss: 0.00014476747310254723\n",
      "Episode Reward: 1.0\n",
      "Step 200 (75124) @ Episode 304/10000, loss: 0.001088903401978314\n",
      "Episode Reward: 1.0\n",
      "Step 100 (75238) @ Episode 305/10000, loss: 2.4395372747676447e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (75509) @ Episode 306/10000, loss: 0.00019226560834795237\n",
      "Episode Reward: 2.0\n",
      "Step 200 (75787) @ Episode 307/10000, loss: 0.0005408109282143414\n",
      "Episode Reward: 1.0\n",
      "Step 300 (76093) @ Episode 308/10000, loss: 0.0006046564085409045\n",
      "Episode Reward: 3.0\n",
      "Step 100 (76235) @ Episode 309/10000, loss: 0.00012127325317123905\n",
      "Episode Reward: 0.0\n",
      "Step 400 (76717) @ Episode 310/10000, loss: 6.713126640534028e-05\n",
      "Episode Reward: 5.0\n",
      "Step 300 (77024) @ Episode 311/10000, loss: 0.0003566453524399549\n",
      "Episode Reward: 3.0\n",
      "Step 200 (77230) @ Episode 312/10000, loss: 0.00046179266064427793\n",
      "Episode Reward: 2.0\n",
      "Step 200 (77467) @ Episode 313/10000, loss: 2.455659159750212e-05\n",
      "Episode Reward: 2.0\n",
      "Step 100 (77622) @ Episode 314/10000, loss: 2.3695400159340352e-05\n",
      "Episode Reward: 0.0\n",
      "Step 100 (77794) @ Episode 315/10000, loss: 0.0007604441489093006\n",
      "Episode Reward: 0.0\n",
      "Step 100 (77965) @ Episode 316/10000, loss: 6.468445644713938e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (78245) @ Episode 317/10000, loss: 8.804908247839194e-06\n",
      "Episode Reward: 1.0\n",
      "Step 200 (78472) @ Episode 318/10000, loss: 0.0006974002462811768\n",
      "Episode Reward: 1.0\n",
      "Step 100 (78589) @ Episode 319/10000, loss: 0.0008243860793299973\n",
      "Episode Reward: 0.0\n",
      "Step 200 (78850) @ Episode 320/10000, loss: 0.0010697654215618968\n",
      "Episode Reward: 2.0\n",
      "Step 200 (79096) @ Episode 321/10000, loss: 8.937659004004672e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (79303) @ Episode 322/10000, loss: 9.301930549554527e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (79548) @ Episode 323/10000, loss: 0.0002800955262500793\n",
      "Episode Reward: 2.0\n",
      "Step 200 (79843) @ Episode 324/10000, loss: 0.005177527666091919\n",
      "Episode Reward: 1.0\n",
      "Step 100 (79990) @ Episode 325/10000, loss: 4.37439521192573e-05\n",
      "Copied model parameters to target network.\n",
      "Step 200 (80090) @ Episode 325/10000, loss: 0.007051704451441765\n",
      "Episode Reward: 1.0\n",
      "Step 200 (80327) @ Episode 326/10000, loss: 0.001815590774640441\n",
      "Episode Reward: 3.0\n",
      "Step 200 (80603) @ Episode 327/10000, loss: 9.13896001293324e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (80828) @ Episode 328/10000, loss: 0.0020356460008770227\n",
      "Episode Reward: 1.0\n",
      "Step 300 (81134) @ Episode 329/10000, loss: 0.001226881518959999\n",
      "Episode Reward: 2.0\n",
      "Step 200 (81351) @ Episode 330/10000, loss: 0.00024020150885917246\n",
      "Episode Reward: 2.0\n",
      "Step 200 (81630) @ Episode 331/10000, loss: 0.00020383106311783195\n",
      "Episode Reward: 2.0\n",
      "Step 100 (81780) @ Episode 332/10000, loss: 0.00022432128025684506\n",
      "Episode Reward: 0.0\n",
      "Step 200 (82049) @ Episode 333/10000, loss: 0.0014588972553610802\n",
      "Episode Reward: 2.0\n",
      "Step 200 (82345) @ Episode 334/10000, loss: 0.000647509703412652\n",
      "Episode Reward: 1.0\n",
      "Step 200 (82584) @ Episode 335/10000, loss: 0.00036891683703288436\n",
      "Episode Reward: 1.0\n",
      "Step 200 (82800) @ Episode 336/10000, loss: 4.592546247295104e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (83027) @ Episode 337/10000, loss: 0.000146109436172992\n",
      "Episode Reward: 2.0\n",
      "Step 100 (83201) @ Episode 338/10000, loss: 4.419479955686256e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (83473) @ Episode 339/10000, loss: 0.00048401643289253116\n",
      "Episode Reward: 2.0\n",
      "Step 200 (83741) @ Episode 340/10000, loss: 0.0033094207756221294\n",
      "Episode Reward: 2.0\n",
      "Step 100 (83896) @ Episode 341/10000, loss: 0.00012376520317047834\n",
      "Episode Reward: 0.0\n",
      "Step 200 (84170) @ Episode 342/10000, loss: 8.079366671154276e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (84403) @ Episode 343/10000, loss: 0.00024694809690117836\n",
      "Episode Reward: 1.0\n",
      "Step 100 (84530) @ Episode 344/10000, loss: 0.00022202846594154835\n",
      "Episode Reward: 0.0\n",
      "Step 100 (84716) @ Episode 345/10000, loss: 0.0005931221530772746\n",
      "Episode Reward: 0.0\n",
      "Step 200 (84987) @ Episode 346/10000, loss: 0.0002904943539761007\n",
      "Episode Reward: 2.0\n",
      "Step 300 (85381) @ Episode 347/10000, loss: 2.575759936007671e-05\n",
      "Episode Reward: 3.0\n",
      "Step 300 (85706) @ Episode 348/10000, loss: 0.00016547944687772542\n",
      "Episode Reward: 2.0\n",
      "Step 200 (85915) @ Episode 349/10000, loss: 0.0002170796215068549\n",
      "Episode Reward: 2.0\n",
      "Step 200 (86190) @ Episode 350/10000, loss: 0.0002630092203617096\n",
      "Episode Reward: 2.0\n",
      "Step 100 (86357) @ Episode 351/10000, loss: 0.0014806939288973808\n",
      "Episode Reward: 0.0\n",
      "Step 400 (86839) @ Episode 352/10000, loss: 1.90704595297575e-05\n",
      "Episode Reward: 5.0\n",
      "Step 100 (86984) @ Episode 353/10000, loss: 0.00020342915377113968\n",
      "Episode Reward: 0.0\n",
      "Step 200 (87269) @ Episode 354/10000, loss: 0.0005942332791164517\n",
      "Episode Reward: 1.0\n",
      "Step 100 (87417) @ Episode 355/10000, loss: 4.651733615901321e-05\n",
      "Episode Reward: 0.0\n",
      "Step 100 (87594) @ Episode 356/10000, loss: 0.00045803640387021005\n",
      "Episode Reward: 0.0\n",
      "Step 300 (87961) @ Episode 357/10000, loss: 0.0010163092520087957\n",
      "Episode Reward: 2.0\n",
      "Step 200 (88163) @ Episode 358/10000, loss: 0.0014521556440740824\n",
      "Episode Reward: 1.0\n",
      "Step 300 (88519) @ Episode 359/10000, loss: 0.0004924082895740867\n",
      "Episode Reward: 2.0\n",
      "Step 200 (88729) @ Episode 360/10000, loss: 4.408095992403105e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (88945) @ Episode 361/10000, loss: 4.1757703002076596e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (89198) @ Episode 362/10000, loss: 0.0002953784423880279\n",
      "Episode Reward: 2.0\n",
      "Step 200 (89490) @ Episode 363/10000, loss: 0.00038004040834493935\n",
      "Episode Reward: 1.0\n",
      "Step 100 (89631) @ Episode 364/10000, loss: 0.00022425653878599405\n",
      "Episode Reward: 0.0\n",
      "Step 200 (89911) @ Episode 365/10000, loss: 6.585979281226173e-05\n",
      "Copied model parameters to target network.\n",
      "Step 300 (90011) @ Episode 365/10000, loss: 2.5421428290428594e-05\n",
      "Episode Reward: 3.0\n",
      "Step 100 (90136) @ Episode 366/10000, loss: 0.00016317216795869172\n",
      "Episode Reward: 0.0\n",
      "Step 300 (90498) @ Episode 367/10000, loss: 0.0023128974717110395\n",
      "Episode Reward: 4.0\n",
      "Step 200 (90793) @ Episode 368/10000, loss: 0.0005988417542539537\n",
      "Episode Reward: 2.0\n",
      "Step 200 (91080) @ Episode 369/10000, loss: 0.0007061208598315716\n",
      "Episode Reward: 1.0\n",
      "Step 200 (91329) @ Episode 370/10000, loss: 0.0013810647651553154\n",
      "Episode Reward: 2.0\n",
      "Step 200 (91610) @ Episode 371/10000, loss: 7.352473039645702e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (91758) @ Episode 372/10000, loss: 0.0003119155880995095\n",
      "Episode Reward: 0.0\n",
      "Step 200 (92026) @ Episode 373/10000, loss: 0.002328070579096675\n",
      "Episode Reward: 1.0\n",
      "Step 100 (92138) @ Episode 374/10000, loss: 0.0011057391529902816\n",
      "Episode Reward: 0.0\n",
      "Step 100 (92325) @ Episode 375/10000, loss: 0.0003274394548498094\n",
      "Episode Reward: 0.0\n",
      "Step 200 (92607) @ Episode 376/10000, loss: 0.0004006075323559344\n",
      "Episode Reward: 1.0\n",
      "Step 200 (92834) @ Episode 377/10000, loss: 0.0026146462187170982\n",
      "Episode Reward: 1.0\n",
      "Step 200 (93077) @ Episode 378/10000, loss: 0.0013258294202387333\n",
      "Episode Reward: 1.0\n",
      "Step 100 (93209) @ Episode 379/10000, loss: 0.0007892561261542141\n",
      "Episode Reward: 0.0\n",
      "Step 200 (93480) @ Episode 380/10000, loss: 0.00013494875747710466\n",
      "Episode Reward: 1.0\n",
      "Step 100 (93621) @ Episode 381/10000, loss: 0.0021052348893135786\n",
      "Episode Reward: 0.0\n",
      "Step 100 (93792) @ Episode 382/10000, loss: 0.003104971954599023\n",
      "Episode Reward: 0.0\n",
      "Step 200 (94080) @ Episode 383/10000, loss: 0.001682985806837678\n",
      "Episode Reward: 2.0\n",
      "Step 100 (94237) @ Episode 384/10000, loss: 0.0015818881802260876\n",
      "Episode Reward: 0.0\n",
      "Step 100 (94425) @ Episode 385/10000, loss: 0.0004732535744551569\n",
      "Episode Reward: 0.0\n",
      "Step 300 (94789) @ Episode 386/10000, loss: 0.00032118114177137613\n",
      "Episode Reward: 3.0\n",
      "Step 200 (95036) @ Episode 387/10000, loss: 0.0003138157771900296\n",
      "Episode Reward: 1.0\n",
      "Step 200 (95280) @ Episode 388/10000, loss: 0.0002635618147905916\n",
      "Episode Reward: 1.0\n",
      "Step 200 (95537) @ Episode 389/10000, loss: 0.00016036782471928746\n",
      "Episode Reward: 1.0\n",
      "Step 100 (95702) @ Episode 390/10000, loss: 0.0003328436869196594\n",
      "Episode Reward: 0.0\n",
      "Step 200 (95974) @ Episode 391/10000, loss: 0.00015956908464431763\n",
      "Episode Reward: 2.0\n",
      "Step 200 (96239) @ Episode 392/10000, loss: 0.0021495744585990906\n",
      "Episode Reward: 2.0\n",
      "Step 200 (96502) @ Episode 393/10000, loss: 0.001766231725923717\n",
      "Episode Reward: 1.0\n",
      "Step 200 (96739) @ Episode 394/10000, loss: 0.0009334111819043756\n",
      "Episode Reward: 1.0\n",
      "Step 400 (97144) @ Episode 395/10000, loss: 0.0012582603376358747\n",
      "Episode Reward: 4.0\n",
      "Step 200 (97359) @ Episode 396/10000, loss: 1.4762947103008628e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (97623) @ Episode 397/10000, loss: 0.006515032611787319\n",
      "Episode Reward: 2.0\n",
      "Step 200 (97919) @ Episode 398/10000, loss: 7.482217188226059e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (98203) @ Episode 399/10000, loss: 0.0005476201185956597\n",
      "Episode Reward: 2.0\n",
      "Step 200 (98473) @ Episode 400/10000, loss: 0.00025329351774416864\n",
      "Episode Reward: 1.0\n",
      "Step 300 (98803) @ Episode 401/10000, loss: 9.566528751747683e-05\n",
      "Episode Reward: 2.0\n",
      "Step 100 (98903) @ Episode 402/10000, loss: 0.0030520055443048477\n",
      "Episode Reward: 0.0\n",
      "Step 100 (99068) @ Episode 403/10000, loss: 4.87772558699362e-05\n",
      "Episode Reward: 0.0\n",
      "Step 200 (99364) @ Episode 404/10000, loss: 7.301774894585833e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (99614) @ Episode 405/10000, loss: 0.0001746482157614082\n",
      "Episode Reward: 2.0\n",
      "Step 200 (99885) @ Episode 406/10000, loss: 2.5727280444698408e-05\n",
      "Episode Reward: 1.0\n",
      "Step 100 (99995) @ Episode 407/10000, loss: 0.0009627209510654211\n",
      "Copied model parameters to target network.\n",
      "Step 200 (100095) @ Episode 407/10000, loss: 0.0008437091019004583\n",
      "Episode Reward: 1.0\n",
      "Step 100 (100254) @ Episode 408/10000, loss: 0.002135128015652299\n",
      "Episode Reward: 0.0\n",
      "Step 100 (100416) @ Episode 409/10000, loss: 0.005504620261490345\n",
      "Episode Reward: 0.0\n",
      "Step 400 (100890) @ Episode 410/10000, loss: 0.008243839256465435\n",
      "Episode Reward: 5.0\n",
      "Step 100 (100993) @ Episode 411/10000, loss: 0.00042590705561451614\n",
      "Episode Reward: 0.0\n",
      "Step 400 (101467) @ Episode 412/10000, loss: 0.006049382500350475\n",
      "Episode Reward: 4.0\n",
      "Step 300 (101773) @ Episode 413/10000, loss: 0.0010894148144870996\n",
      "Episode Reward: 4.0\n",
      "Step 400 (102242) @ Episode 414/10000, loss: 0.0017246801871806383\n",
      "Episode Reward: 4.0\n",
      "Step 200 (102473) @ Episode 415/10000, loss: 0.0003596009046304971\n",
      "Episode Reward: 1.0\n",
      "Step 300 (102815) @ Episode 416/10000, loss: 0.0007425969233736396\n",
      "Episode Reward: 4.0\n",
      "Step 200 (103111) @ Episode 417/10000, loss: 0.0017422860255464911\n",
      "Episode Reward: 1.0\n",
      "Step 200 (103354) @ Episode 418/10000, loss: 0.0018001204589381814\n",
      "Episode Reward: 1.0\n",
      "Step 100 (103463) @ Episode 419/10000, loss: 0.0005042112316004932\n",
      "Episode Reward: 0.0\n",
      "Step 200 (103738) @ Episode 420/10000, loss: 0.0006812482024542987\n",
      "Episode Reward: 1.0\n",
      "Step 200 (103976) @ Episode 421/10000, loss: 0.0003846234758384526\n",
      "Episode Reward: 0.0\n",
      "Step 100 (104080) @ Episode 422/10000, loss: 0.0004951130249537528\n",
      "Episode Reward: 0.0\n",
      "Step 200 (104355) @ Episode 423/10000, loss: 0.0018547526560723782\n",
      "Episode Reward: 2.0\n",
      "Step 200 (104606) @ Episode 424/10000, loss: 0.00027578911976888776\n",
      "Episode Reward: 1.0\n",
      "Step 400 (105060) @ Episode 425/10000, loss: 1.9998327843495645e-05\n",
      "Episode Reward: 4.0\n",
      "Step 100 (105169) @ Episode 426/10000, loss: 0.0035249667707830667\n",
      "Episode Reward: 0.0\n",
      "Step 200 (105440) @ Episode 427/10000, loss: 0.0004581288085319102\n",
      "Episode Reward: 2.0\n",
      "Step 300 (105832) @ Episode 428/10000, loss: 0.0006096658762544394\n",
      "Episode Reward: 3.0\n",
      "Step 200 (106071) @ Episode 429/10000, loss: 2.796243643388152e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (106320) @ Episode 430/10000, loss: 0.0018677315674722195\n",
      "Episode Reward: 1.0\n",
      "Step 200 (106566) @ Episode 431/10000, loss: 0.0002782227529678494\n",
      "Episode Reward: 2.0\n",
      "Step 200 (106835) @ Episode 432/10000, loss: 0.00018735886260401458\n",
      "Episode Reward: 2.0\n",
      "Step 300 (107209) @ Episode 433/10000, loss: 8.324874943355098e-05\n",
      "Episode Reward: 2.0\n",
      "Step 200 (107414) @ Episode 434/10000, loss: 0.0009856335818767548\n",
      "Episode Reward: 1.0\n",
      "Step 200 (107625) @ Episode 435/10000, loss: 2.811537888192106e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (107847) @ Episode 436/10000, loss: 7.192806515377015e-05\n",
      "Episode Reward: 1.0\n",
      "Step 200 (108094) @ Episode 437/10000, loss: 0.00048445657012052834\n",
      "Episode Reward: 1.0\n",
      "Step 200 (108318) @ Episode 438/10000, loss: 0.0009010492358356714\n",
      "Episode Reward: 2.0\n",
      "Step 200 (108590) @ Episode 439/10000, loss: 0.0015523576876148582\n",
      "Episode Reward: 2.0\n",
      "Step 200 (108851) @ Episode 440/10000, loss: 0.0002769201819319278\n",
      "Episode Reward: 1.0\n",
      "Step 100 (109007) @ Episode 441/10000, loss: 0.00014681757602375\n",
      "Episode Reward: 0.0\n",
      "Step 300 (109394) @ Episode 442/10000, loss: 0.00013852206757292151\n",
      "Episode Reward: 3.0\n",
      "Step 100 (109538) @ Episode 443/10000, loss: 0.000108760577859357\n",
      "Episode Reward: 0.0\n",
      "Step 100 (109722) @ Episode 444/10000, loss: 0.001242274884134531"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-44ee54e03d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ad1180728ae1>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mq_value_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-01afd82e1698>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "# q_estimator: keep updating the q-value function\n",
    "# target_estimator: copy from q_estimator every N steps, used to \n",
    "# predict next stage q-value\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    #replay_memory_init_size=50,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
